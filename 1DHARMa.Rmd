---
title: 'Statistically Speaking 1: Diagnosing Assumption Violations in General(ized)
  Linear Models with DHARMa in R'
author: "Zachary Beneduci"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggpubr)
```

## Purpose

Welcome to the first installment of the Statistically Speaking... blog? Tutorials? Not sure what this will morph into in the long run. What I am sure of is that many folks struggle with statistics - so much so that they may take a non-trivial amount of university courses and still feel like an ostrich with its head in the sand. Having taken a few of these courses myself, one area that is woefully neglected is that of checking model assumptions. Often, students are shown how to check assumptions for the general linear model (often abbreviated LM). Yet, this is not so straightforward for general**ized** linear (mixed) models (GL(M)Ms), and requires a departure from the standard residual plots. I hope that by the end of this document you'll feel a bit more confident that the GL(M)Ms you fit agree with your data.

## An overview of linear models

Before I get into the main topic of the document, I think a refresher on linear models would serve the reader well. I'll go over:

1.  the general linear model
2.  fixed vs. random effects (as these are commonly used together in ecology)
3.  and generalized linear models

### The General Linear Model

Recall from your early algebra classes the slope-intercept form of the linear equation, which follows:

Eqn. 1a: $y = mx + b$,

where $y$ is the value of the dependent variable, $x$ is value of the independent variable, $b$ is the y-intercept (the value at which the line crossed the y-axis), and $m$ is the slope (the change in $y$ for every unit of $x$).

For example, if $m = 2.6$ is the slope, $b = 3.4$ is the y-intercept, and I want to know the value of $y$ when $x = 4$, we can solve the following equation for $y$:

Eqn. 1b: $y = 2.6*4 + 3.4$

```{r include=FALSE}
eqn1b <- (2.6*4)+3.4
```

which gives us `r eqn1b`.

Now, this is all well and good if you know the values for the variable $x$ and coefficients ($m$ and $b$). But let's say our information is reversed: we have a dataset, either collected in the field or provided by a colleague, where we have some number of paired values of $x$ and $y$. And I want to know how $x$ relates to $y$ (i.e. the slope, $m$) and I similarly don't know $b$. Trying to figure this out with only Equation 1 and by hand would be insane. What, am I supposed to pick a pair of $x$ and $y$ values and arbitrarily select values of coefficients $m$ and $b$ until I find a solution? Doing this for more than one pair becomes infinitely daunting, especially since the coefficient values need to work for all possible variable pairs. Luckily, there are techniques to *estimate* what these coefficients should be.

This brings us to the linear model, which uses a technique called linear regression to relate (typically) one $y$ to one or more $x$s. In linear modeling, the linear equation is represented a bit differently:

Eqn. 2a. $y = \beta_0 + \beta_1X_1 + \epsilon$,

where $y$ is still the independent variable, $X_1$ is the dependent variable, $\beta_0$ (read beta-0) which was $b$ is now the y-intercept, and $\beta_1$ is which was $m$ is now the slope. You'll also notice that there is this term $\epsilon$ (read epsilon) that stands for the residual error. You might be asking, what is the residual error?

I'll illustrate by creating some data where we know the true coefficient values.

```{r}
set.seed(123)
datum <- data.frame(X = runif(n = 30, min = 0, max = 10)) %>%
  mutate(error = rnorm(n = 30, mean = 0, sd = 2.84),
         y = 4.32 + 1.67*X + error)
```

Plotting these data:

```{r, fig.width=3, fig.height=3, fig.retina=600/72}
plot1 <- ggplot(datum, aes(X, y)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "lm", se = T) +
  theme_bw()

plot1
```

And here we have our simulated data.

Notice, I've also fitted a line through them. The coefficients for this line were estimated with an approach called Ordinary Least Squares regression (OLS). OLS is a common approach to estimate values of the coefficients that are most likely to produce the observed data, assuming that the assumptions of the LM are met (more on that later). Two steps are required to do this. The first is that the average error of the model equals zero. We call this the total sum of squares ($TSS$):

Eqn. 3a: $TSS = \sum_{i=1}^{n} (y_i - \bar{y})^2$

where $TSS$ represents the total error that could possibly be explained by the model, $y_i$ (read y sub i) is each $y$ value indexed ($i$) from observation 1 through the total number of observations ($n$), $\bar{y}$ (read y-bar) is the average $y$ value across the whole dataset, and $y_i - \bar{y}$ represents the distance of each observation from the average of all obervations. We square each of these differences and sum them together to get $TSS$, and the model finds coefficient values that make $TSS = 0$.

However, this isn't enough to give us our line. There are many possible combinations of $\beta_0$ and $\beta_1$ that would make $TSS = 0$. Another requirement, then, is that the sum of squares error ($SSE$) is minimized:

Eqn. 3b: $SSE = \sum_{i=1}^{n} (y_i - \hat{y})^2$

All we've done is swap $\bar{y}$ for $\hat{y}$, the latter the predicted y-value that the same x-value of observation $y_i$. So, if $y_1$ occurs at $x = 1.2$, $\hat{y}$ is the predicted value at $x = 1.2$. This gives us the error that is **unexplained** by the line. So, minimizing the amount of error that is not explained by the line will ensure that the line runs through the middle of our observations. Note that this is the "residual error" being talked about in the context of model assumptions.

While this is technically enough to understand OLS regression, there is one more component: sum of squares regression ($SSR$), or the error **explained** by the line:

Eqn. 3c: $SSR = \sum_{i=1}^{n} (\hat{y_i} - \bar{y})^2$

Now, we subtract the mean y-value across the dataset ($\bar{y}$) from the predicted y-value ($\hat{y}$) at each observed x-value. Put another way, this is the distance of each predicted y-value from the average y-value. Interestingly, there is a relation between these three types of errors:

Eqn. 3d: $TSS = SSE + SSR$

The error unexplained by the line ($SSE$) and the error explained by the line ($SSR$) add up to the total error in the dataset ($TSS$).

Let's visualize this for a single data point to really drive it home:

```{r, fig.width=3, fig.height=3, fig.retina=600/72}
# Caluclate the average y:
mean.y <- summarise(datum, mean = mean(y))

# Filter the dataset down to a focal observation:
datum2 <- datum %>%
  mutate(row = row_number()) %>%
  filter(row == 2)

m1 <- lm(y~X, datum)

pt <- 7.883051
pt.pred <- 5.9009 + (1.4059*7.883051)

ggplot(datum, aes(X, y)) +
  geom_hline(yintercept = mean.y[1,1]) +
  geom_segment(x = pt, xend = pt, y = datum2[1,3], yend = pt.pred, color = "red") +
  geom_segment(x = pt, xend = pt, y = mean.y[1,1], yend = pt.pred, color = "green") +
  geom_point(datum2, mapping = aes(X,y), inherit.aes = F) +
  geom_smooth(method = "lm", se = F) +
  theme_bw()
```

The black horizontal line is the average y-value ($\bar{y}$). In red is the $SSE$ and green the $SSR$.

The portion of the $SSE$ equation $y_i - \hat{y}$ is typically referred to as the **residual error**, and each value for observation $i$ is called a **residual**. I'll refer to them as this from now on. Note that as I've calculated them here, they are **raw residuals** that have not undergone some kind of transformation, which you will see later.

Naturally, you might be curious how well the OLS estimated our true line. The true coefficients were set at $\beta_0 = 4.32$ and $\beta_1 = 1.67$.

Let's see how the OLS stacked up:

```{r}
m1 <- lm(y~X, data = datum)

m1.sum <- summary(m1)

m1.sum$coefficients %>%
  as.data.frame() %>%
  knitr::kable()
```

These estimates aren't that far off given that I added a fair bit of noise around each observation. I did this by simulating some error around each observation from a normal distribution with a mean of $\mu = 0$ and variance of $\sigma = 2.84$. That variance is fairly high given the size of the coefficients I set.

```{r, fig.width=3, fig.height=3, fig.retina=600/72}
plot1 +
  geom_abline(intercept = 4.32, slope = 1.67, linewidth = 1)
```

The band around the line denotes the 95% confidence interval. This is a region where the model expects, given the variation in the data, that the true line falls. We expect that 95% of all confidence intervals generated will contain truth. Put another way, if we generated 100 confidence intervals, we would expect that about 95% of them would contain the true values of the coefficients. Our truth (black line) is within or 95% C.I., so the OLS is doing a pretty good job.

However, there are times when the OLS will not do a good job. The linear model performs well **given that the data meet a set of assumptions** specific to the general linear model. Briefly, those are:

1.  The dependent variable ($y$) is continuous
2.  The residual error is normally distributed
3.  There is a linear relationship between $x$ and $y$
4.  The variance is homoskedastic
5.  Samples/observations are independent; there is no autocorrelation

Violation of any one of these can 1) **bias the estimated parameters** (i.e. $\beta$s) away from the true value and 2) **decrease the precision** around the estimate (think variance, standard error, confidence intervals, and p-values). As such, it's incredibly important to check whether these assumptions have been violated.

And the best way to do that is by checking the residuals!

Luckily, this is fairly straightforward for LMs - facilitated through various plots.

### Checking Assumptions of the Linear Model

#### Continuous $y$ 

Our first assumption that the dependent variable is continuous is the easiest to check. Here, we're making sure that the dependent, or response variable, is a number that could contain decimals. Think height measurements that could be 1.54, 2.76, 8.94, etc... These can be both positive and negative numbers. Violations of this would be data that are discrete counts (1, 2, 3, etc...), proportions that can only range between 0 and 1 (i.e. 0.3, 0.5, 0.8, etc...), and binary response variables (yes vs. no, 1 vs. 0).

#### Normally distributed residual error

Here we come to our first assumption that is best to check with some kind of residual plot.

You might also be interested to see the residuals on the same plot:

```{r, fig.width=3, fig.height=3, fig.retina=600/72}
m1.res <- residuals(m1)

datum <- datum %>%
  mutate(res = m1.res,
         y.ols = m1.sum$coefficients[1,1] + (m1.sum$coefficients[2,1]*X))

ggplot(datum, aes(X, y)) +
  geom_point() +
  geom_segment(aes(x = X, y = y.ols + res, xend = X, yend = y.ols), color = "red") +
  geom_smooth(method = "lm", se = T) +
  theme_bw()
```

As you can see, extracting the residuals from the model and adding these values to the fitted line gives the location of each observation.
